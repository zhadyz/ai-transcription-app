"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TRANSCENDENT TRANSCRIPTION ENGINE - Maximum Enlightenment
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Features:
- Zero-copy streaming upload support
- Real-time WebSocket progress broadcasting
- Intelligent file matching (streaming flow)
- Comprehensive error handling
- Automatic resource cleanup
- GPU-accelerated Whisper transcription
- Multi-format export (SRT, VTT, TXT, CSV, JSON)
"""

from fastapi import APIRouter, UploadFile, HTTPException, BackgroundTasks, Form, Request, Depends, Body
from fastapi.responses import FileResponse
from pydantic import BaseModel
from typing import Dict, Literal, Optional
import uuid
import asyncio
import time
import os
import shutil
import logging
from pathlib import Path

from app.models.transcription import (
    TranscriptionProgress,
    TranscriptionResult,
    TranscriptionRequest,
    Segment,
    ExportFormat,
    TranscriptionQuality,
    TranscriptionLanguage
)
from app.services.audio_service import AudioService
from app.services.whisper_service import WhisperService
from app.services.export_service import ExportService
from app.services.validation_service import validate_file_type, validate_file_size, sanitize_filename
from app.middleware.rate_limit import limiter
from app.middleware.rate_limit_config import TRANSCRIPTION_UPLOAD_LIMIT
from app.middleware.auth import maybe_require_api_key
from app.config import settings

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/transcribe", tags=["transcription"])

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# IN-MEMORY TASK STORAGE (Use Redis in production for persistence)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

tasks: Dict[str, TranscriptionProgress] = {}
task_results: Dict[str, TranscriptionResult] = {}
task_files: Dict[str, str] = {}
task_to_session: Dict[str, str] = {}  # Map task_id to session_id

# Concurrency control - limit simultaneous transcriptions
transcription_semaphore = asyncio.Semaphore(settings.MAX_CONCURRENT_TRANSCRIPTIONS)

# Service instances
audio_service = AudioService()
whisper_service = WhisperService()
export_service = ExportService()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PYDANTIC MODELS FOR REQUEST VALIDATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class TranscriptionStartRequest(BaseModel):
    """Request model for starting transcription after streaming upload"""
    filename: str
    quality: Literal["base", "small", "medium", "large-v2", "large-v3"] = "small"
    language: str = "auto"
    export_format: Literal["srt", "vtt", "txt", "csv", "json"] = "srt"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# UTILITY FUNCTIONS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def estimate_processing_time(duration_seconds: float, quality: str) -> int:
    """
    Estimate transcription processing time based on audio duration and model quality.
    
    Args:
        duration_seconds: Audio duration in seconds
        quality: Model quality level (base, small, medium)
        
    Returns:
        Estimated processing time in seconds
    """
    quality_multipliers = {
        "base": 0.5,   # Fastest, least accurate
        "small": 1.0,  # Balanced
        "medium": 2.0  # Slowest, most accurate
    }
    multiplier = quality_multipliers.get(quality, 1.0)
    return int(duration_seconds * multiplier)


async def update_progress(
    task_id: str,
    status: str,
    progress: float,
    current_step: str,
    estimated_time: int = None,
    session_id: Optional[str] = None
):
    """
    Update task progress and broadcast via WebSocket if session exists.
    
    Eliminates need for frontend polling by pushing updates in real-time.
    
    Args:
        task_id: Unique task identifier
        status: Task status (queued, processing, completed, failed)
        progress: Progress percentage (0-100)
        current_step: Human-readable current step description
        estimated_time: Estimated seconds remaining
        session_id: Optional session ID for WebSocket broadcasting
    """
    tasks[task_id] = TranscriptionProgress(
        task_id=task_id,
        status=status,
        progress=progress,
        current_step=current_step,
        estimated_time_remaining=estimated_time
    )
    
    # Broadcast progress via WebSocket if session is linked
    if session_id:
        try:
            from app.api.routes.websocket import notify_session
            await notify_session(session_id, {
                "type": "progress_update",
                "task_id": task_id,
                "status": status,
                "progress": progress,
                "current_step": current_step,
                "estimated_time_remaining": estimated_time
            })
            logger.debug(f"ğŸ“¡ Progress broadcast: {task_id[:8]} -> {progress:.1f}%")
        except Exception as e:
            logger.warning(f"Failed to broadcast progress: {e}")


async def process_transcription(
    task_id: str,
    file_path: Path,
    settings_param: TranscriptionRequest,
    session_id: Optional[str] = None
):
    """
    ğŸ¯ ULTIMATE TRANSCENDENT TRANSCRIPTION PIPELINE
    
    Handles files from 1KB to 5GB+ with constant memory usage.
    Fully async, non-blocking, production-grade.
    
    Architecture:
    - Async file I/O with aiofiles
    - Thread pool for CPU-bound operations (FFmpeg, Whisper)
    - Chunked processing for large files
    - Real-time progress broadcasting
    - Automatic resource cleanup
    
    Pipeline:
        1. Extract audio from video/audio file (0-10%)
        2. Transcribe with Whisper model (10-85%)
        3. Export to requested format (85-95%)
        4. Finalize and cleanup (95-100%)
    
    Args:
        task_id: Unique task identifier
        file_path: Path to uploaded file
        settings_param: Transcription settings (language, quality, format)
        session_id: Optional session ID for WebSocket broadcasting
    """
    logger.info(
        f"ğŸ¤ Processing transcription: {task_id[:8]} "
        f"(session: {session_id[:8] if session_id else 'none'})"
    )
    
    # Store task-to-session mapping if session provided
    if session_id:
        task_to_session[task_id] = session_id
    
    # Wait for semaphore - limits concurrent transcriptions
    async with transcription_semaphore:
        audio_path = None
        try:
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # STEP 1: EXTRACT AUDIO (0-10%)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            logger.info(f"ğŸ“Š Step 1/4: Extracting audio for {task_id[:8]}")
            await update_progress(
                task_id, "processing", 0.0, "Extracting audio...", None, session_id
            )
            
            # Run FFmpeg in thread pool (CPU/IO bound)
            audio_path = await asyncio.to_thread(
                audio_service.extract_audio, str(file_path), task_id
            )
            
            # Get audio duration (quick operation)
            audio_duration = await asyncio.to_thread(
                audio_service.get_audio_duration, audio_path
            )
            
            estimated_total_time = estimate_processing_time(
                audio_duration, settings_param.quality.value
            )
            if estimated_total_time == 0:
                estimated_total_time = 5
            
            await update_progress(
                task_id, "processing", 10.0, "Audio extracted",
                estimated_total_time, session_id
            )
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # STEP 2: TRANSCRIBE (10-85%)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            logger.info(f"ğŸ¤ Step 2/4: Transcribing audio for {task_id[:8]}")
            await update_progress(
                task_id, "processing", 10.0, "Starting transcription...",
                estimated_total_time, session_id
            )
            
            start_time = time.time()
            lang_arg = None if settings_param.language == TranscriptionLanguage.AUTO else settings_param.language.value
            
            # Background task to simulate progress during transcription
            transcription_done = False
            
            async def simulate_progress():
                """Simulate smooth progress during long transcription"""
                current = 10.0
                while not transcription_done and current < 85.0:
                    elapsed = time.time() - start_time
                    if estimated_total_time > 0:
                        time_based_progress = min((elapsed / estimated_total_time) * 75, 75)
                        current = 10 + time_based_progress
                        remaining = max(0, int(estimated_total_time - elapsed))
                        await update_progress(
                            task_id, "processing", min(current, 84.9),
                            "Transcribing...", remaining, session_id
                        )
                    await asyncio.sleep(1)
            
            # Start progress simulation
            progress_task = asyncio.create_task(simulate_progress())
            
            # Run actual transcription in thread pool (GPU/CPU bound)
            full_text, detected_lang, segments = await asyncio.to_thread(
                whisper_service.transcribe, audio_path, settings_param.quality, lang_arg
            )
            
            # Stop progress simulation
            transcription_done = True
            await progress_task
            
            logger.info(f"âœ… Transcription complete: {len(segments)} segments")
            await update_progress(
                task_id, "processing", 85.0, "Transcription complete", 5, session_id
            )
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # STEP 3: EXPORT (85-95%)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            logger.info(f"ğŸ“ Step 3/4: Exporting {settings_param.export_format.value.upper()}")
            await update_progress(
                task_id, "processing", 85.0,
                f"Exporting {settings_param.export_format.value.upper()}...",
                3, session_id
            )
            
            # Generate export content (in-memory)
            export_content = export_service.export(segments, settings_param.export_format)
            
            # Ensure transcript directory exists
            transcript_dir = Path("./storage/transcripts")
            await asyncio.to_thread(transcript_dir.mkdir, parents=True, exist_ok=True)
            
            export_filename = f"{task_id}.{settings_param.export_format.value}"
            export_path = transcript_dir / export_filename
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # ğŸ¯ TRANSCENDENT FILE WRITING
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # Use aiofiles for consistent async I/O
            # Small files (SRT/VTT): writes instantly
            # Large files (hypothetically): non-blocking
            
            import aiofiles
            
            async with aiofiles.open(export_path, "w", encoding="utf-8") as f:
                await f.write(export_content)
            
            await update_progress(task_id, "processing", 95.0, "Finalizing...", 1, session_id)
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # STEP 4: CREATE RESULT (95-100%)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            result = TranscriptionResult(
                task_id=task_id,
                text=" ".join(s.text.strip() for s in segments),
                language_detected=detected_lang if segments else settings_param.language.value,
                segments=segments,
                download_url=f"/transcribe/download/{task_id}",
                export_format=settings_param.export_format
            )
            
            task_results[task_id] = result
            task_files[task_id] = str(export_path)
            
            await update_progress(
                task_id, "completed", 100.0, "Transcription complete!", None, session_id
            )
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # BROADCAST COMPLETION TO WEBSOCKET
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            if session_id:
                from app.api.routes.websocket import notify_session
                await notify_session(session_id, {
                    "type": "transcription_completed",
                    "task_id": task_id,
                    "status": "completed",
                    "result": {
                        "text": result.text,
                        "segments": [
                            {
                                "start": s.start,
                                "end": s.end,
                                "text": s.text
                            } for s in result.segments
                        ],
                        "language_detected": result.language_detected
                    },
                    "sessionId": session_id
                })
                logger.info(f"âœ… Broadcast transcription_completed for task {task_id[:8]}")
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # CLEANUP - Async file deletion
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            if audio_path:
                await asyncio.to_thread(audio_service.cleanup, audio_path)
            
            if file_path.exists():
                await asyncio.to_thread(file_path.unlink)
            
            logger.info(f"ğŸŠ Task {task_id[:8]} completed successfully")
                
        except Exception as e:
            logger.error(f"âŒ Error processing {task_id[:8]}: {str(e)}", exc_info=True)
            await update_progress(
                task_id, "failed", 0.0, f"Error: {str(e)}", None, session_id
            )
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # ERROR CLEANUP
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            try:
                if audio_path and Path(audio_path).exists():
                    await asyncio.to_thread(audio_service.cleanup, audio_path)
                if file_path.exists():
                    await asyncio.to_thread(file_path.unlink)
            except Exception as cleanup_err:
                logger.warning(f"Cleanup after error failed: {cleanup_err}")
            
            raise
        finally:
            # Cleanup task-to-session mapping
            if task_id in task_to_session:
                del task_to_session[task_id]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ENDPOINTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@router.post("/upload")
@limiter.limit(TRANSCRIPTION_UPLOAD_LIMIT)
async def upload_file(
    request: Request,
    background_tasks: BackgroundTasks,
    file: UploadFile,
    language: str = "auto",
    quality: Literal["base", "small", "medium", "large-v2", "large-v3"] = "small",
    export_format: Literal["srt", "vtt", "txt", "csv", "json"] = "srt",
    _auth: Optional[str] = Depends(maybe_require_api_key)
):
    """
    Upload audio/video file for transcription (desktop clients).
    
    Rate limit: 10 uploads per hour per IP.
    Authentication: Required if REQUIRE_AUTH=True in config.
    
    Supported formats: MP3, WAV, M4A, MP4, AVI, MOV, FLAC, OGG
    Max file size: Configured in settings.MAX_FILE_SIZE_MB
    
    Args:
        file: Audio or video file
        language: Language code or 'auto' for detection
        quality: Model quality (base=fast, small=balanced, medium=accurate)
        export_format: Output format (srt, vtt, txt, csv, json)
        
    Returns:
        Task ID for progress tracking
        
    Raises:
        400: Invalid file type or size
        429: Rate limit exceeded
        500: Upload or processing failure
    """
    # Sanitize filename to prevent path traversal
    safe_filename = sanitize_filename(file.filename)
    file_ext = Path(safe_filename).suffix.lower()
    
    # Basic extension check
    allowed_extensions = {'.mp3', '.wav', '.m4a', '.mp4', '.avi', '.mov', '.flac', '.ogg'}
    if file_ext not in allowed_extensions:
        raise HTTPException(400, f"Unsupported file extension: {file_ext}")
    
    task_id = str(uuid.uuid4())
    upload_dir = Path("./storage/uploads")
    upload_dir.mkdir(parents=True, exist_ok=True)
    
    file_path = upload_dir / f"{task_id}{file_ext}"
    
    try:
        # Save file
        with open(file_path, "wb") as f:
            shutil.copyfileobj(file.file, f)
        
        # Verify file was saved
        if not file_path.exists():
            raise Exception("File was not saved")
        
        # Validate file size
        is_valid_size, size_message = validate_file_size(file_path, settings.MAX_FILE_SIZE_MB)
        if not is_valid_size:
            file_path.unlink()
            raise HTTPException(400, size_message)
        
        # Validate file content (magic bytes check)
        is_valid_type, mime_type, type_message = validate_file_type(file_path)
        if not is_valid_type:
            file_path.unlink()
            raise HTTPException(400, type_message)
        
        file_size = file_path.stat().st_size
        logger.info(
            f"âœ… File validated and saved: {safe_filename} "
            f"({file_size} bytes, {mime_type})"
        )
            
    except HTTPException:
        raise
    except Exception as e:
        if file_path.exists():
            file_path.unlink()
        logger.error(f"Upload failed: {e}")
        raise HTTPException(500, f"Upload failed: {e}")
    
    # Create task
    tasks[task_id] = TranscriptionProgress(
        task_id=task_id,
        status="queued",
        progress=0.0,
        current_step="Queued for processing..."
    )
    
    quality_map = {
    "base": TranscriptionQuality.FAST,
    "small": TranscriptionQuality.BALANCED,
    "medium": TranscriptionQuality.DEEP,
    "large-v2": TranscriptionQuality.ULTRA,
    "large-v3": TranscriptionQuality.MAXIMUM
}
    
    format_map = {
        "srt": ExportFormat.SRT,
        "vtt": ExportFormat.VTT,
        "txt": ExportFormat.TXT,
        "csv": ExportFormat.CSV,
        "json": ExportFormat.JSON
    }
    
    lang_enum = TranscriptionLanguage.AUTO if language == "auto" else TranscriptionLanguage(language)
    transcription_settings = TranscriptionRequest(
        language=lang_enum,
        quality=quality_map.get(quality, TranscriptionQuality.BALANCED),
        export_format=format_map.get(export_format, ExportFormat.SRT)
    )
    
    # Start background processing
    background_tasks.add_task(
        process_transcription, task_id, file_path, transcription_settings, None
    )
    
    return {
        "task_id": task_id,
        "status": "queued",
        "message": "File uploaded successfully. Use task_id to check progress."
    }


@router.post("/start")
async def start_transcription(
    background_tasks: BackgroundTasks,
    request: TranscriptionStartRequest
):
    """
    ğŸš€ TRANSCENDENT START ENDPOINT
    
    Start transcription for an already-uploaded file (streaming upload flow).
    Called after /stream/upload completes successfully.
    
    Validates file size and type before starting transcription.
    """
    
    logger.info(f"ğŸ¬ Starting transcription: {request.filename}")
    logger.info(f"   Quality: {request.quality}, Language: {request.language}, Format: {request.export_format}")
    
    # Find the uploaded file in storage/uploads
    upload_dir = Path("./storage/uploads")
    
    # Get file extension from filename
    file_ext = Path(request.filename).suffix.lower()
    
    if not file_ext:
        raise HTTPException(400, "Filename must have an extension")
    
    # Find most recent file with matching extension (uploaded in last 60 seconds)
    matching_files = []
    current_time = time.time()
    
    for file_path_iter in upload_dir.glob(f"*{file_ext}"):
        file_age = current_time - file_path_iter.stat().st_mtime
        if file_age < 60:  # Only consider files from last minute
            matching_files.append((file_path_iter, file_age))
    
    if not matching_files:
        logger.error(f"âŒ No uploaded file found for: {request.filename}")
        raise HTTPException(
            status_code=404,
            detail=f"No recently uploaded file found with extension {file_ext}. Please upload the file first."
        )
    
    # Get the most recent file
    file_path = min(matching_files, key=lambda x: x[1])[0]
    logger.info(f"ğŸ“ Found uploaded file: {file_path.name} (age: {min(matching_files, key=lambda x: x[1])[1]:.1f}s)")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ğŸ¯ VALIDATE FILE BEFORE TRANSCRIPTION (CRITICAL!)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    logger.info(f"ğŸ” Validating file size (max: {settings.MAX_FILE_SIZE_MB}MB)...")
    
    # Validate file size (uses settings.MAX_FILE_SIZE_MB)
    is_valid_size, size_message = validate_file_size(file_path)
    if not is_valid_size:
        logger.error(f"âŒ File size validation failed: {size_message}")
        file_path.unlink()  # Delete oversized file
        raise HTTPException(400, size_message)
    
    # Validate file type (magic bytes check) - NOW ENABLED with retry logic
    is_valid_type, mime_type, type_message = validate_file_type(file_path)
    if not is_valid_type:
        logger.error(f"âŒ File type validation failed: {type_message}")
        file_path.unlink()  # Delete invalid file
        raise HTTPException(400, type_message)
    
    logger.info(f"âœ… File validated: {mime_type}, {size_message}")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CREATE TRANSCRIPTION TASK
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    task_id = str(uuid.uuid4())
    
    # Map settings
    quality_map = {
        "base": TranscriptionQuality.FAST,
        "small": TranscriptionQuality.BALANCED,
        "medium": TranscriptionQuality.DEEP
    }
    
    format_map = {
        "srt": ExportFormat.SRT,
        "vtt": ExportFormat.VTT,
        "txt": ExportFormat.TXT,
        "csv": ExportFormat.CSV,
        "json": ExportFormat.JSON
    }
    
    lang_enum = TranscriptionLanguage.AUTO if request.language == "auto" else TranscriptionLanguage(request.language)
    transcription_settings = TranscriptionRequest(
        language=lang_enum,
        quality=quality_map.get(request.quality, TranscriptionQuality.BALANCED),
        export_format=format_map.get(request.export_format, ExportFormat.SRT)
    )
    
    # Create task entry
    tasks[task_id] = TranscriptionProgress(
        task_id=task_id,
        status="queued",
        progress=0.0,
        current_step="Starting transcription..."
    )
    
    # Submit to background task
    background_tasks.add_task(
        process_transcription,
        task_id=task_id,
        file_path=file_path,
        settings_param=transcription_settings,
        session_id=None
    )
    
    logger.info(f"âœ… Transcription task created: {task_id[:8]}")
    
    return {
        "task_id": task_id,
        "filename": request.filename,
        "status": "queued"
    }


@router.post("/from-path")
async def transcribe_from_path(
    background_tasks: BackgroundTasks,
    file_path: str = Form(...),
    filename: str = Form(...),
    language: str = Form("auto"),
    quality: Literal["base", "small", "medium", "large-v2", "large-v3"] = Form("small"),
    export_format: Literal["srt", "vtt", "txt", "csv", "json"] = Form("srt"),
    session_id: Optional[str] = Form(None),
    _auth: Optional[str] = Depends(maybe_require_api_key)
):
    """
    Transcribe an already uploaded file using its server path.
    
    Used internally after mobile uploads to session.
    
    Args:
        file_path: Server path to uploaded file
        filename: Original filename
        language: Language code or 'auto'
        quality: Model quality level
        export_format: Output format
        session_id: Optional session for WebSocket updates
        
    Returns:
        Task ID for progress tracking
    """
    path = Path(file_path)
    
    if not path.exists():
        raise HTTPException(404, f"File not found: {file_path}")
    
    file_size = path.stat().st_size
    if file_size == 0:
        raise HTTPException(400, "File is empty")
    
    logger.info(f"ğŸ“‚ Transcribing file: {filename} ({file_size} bytes)")
    
    task_id = str(uuid.uuid4())
    
    tasks[task_id] = TranscriptionProgress(
        task_id=task_id,
        status="queued",
        progress=0.0,
        current_step="Starting transcription..."
    )
    
    # Map settings
    quality_map = {
        "base": TranscriptionQuality.FAST,
        "small": TranscriptionQuality.BALANCED,
        "medium": TranscriptionQuality.DEEP
    }
    
    format_map = {
        "srt": ExportFormat.SRT,
        "vtt": ExportFormat.VTT,
        "txt": ExportFormat.TXT,
        "csv": ExportFormat.CSV,
        "json": ExportFormat.JSON
    }
    
    lang_enum = TranscriptionLanguage.AUTO if language == "auto" else TranscriptionLanguage(language)
    transcription_settings = TranscriptionRequest(
        language=lang_enum,
        quality=quality_map.get(quality, TranscriptionQuality.BALANCED),
        export_format=format_map.get(export_format, ExportFormat.SRT)
    )
    
    background_tasks.add_task(
        process_transcription, task_id, path, transcription_settings, session_id
    )
    
    return {
        "task_id": task_id,
        "status": "queued",
        "message": "Transcription started"
    }


@router.get("/progress/{task_id}")
async def get_progress(task_id: str):
    """
    Get transcription progress (HTTP polling - WebSocket preferred).
    
    Args:
        task_id: Task identifier from upload response
        
    Returns:
        Current progress status
        
    Raises:
        404: Task not found
    """
    if task_id not in tasks:
        raise HTTPException(404, "Task not found")
    
    return tasks[task_id]


@router.get("/result/{task_id}")
async def get_result(task_id: str):
    """
    Get completed transcription result.
    
    Args:
        task_id: Task identifier
        
    Returns:
        Transcription text, segments, and metadata
        
    Raises:
        404: Task not found or result not available
        400: Transcription not completed
    """
    if task_id not in tasks:
        raise HTTPException(404, "Task not found")
    
    if tasks[task_id].status != "completed":
        raise HTTPException(400, "Transcription not completed")
    
    if task_id not in task_results:
        raise HTTPException(404, "Result not found")
    
    return task_results[task_id]


@router.get("/download/{task_id}")
async def download_file(task_id: str):
    """
    Download completed transcription file.
    
    Automatically cleans up all associated files after download.
    
    Args:
        task_id: Task identifier
        
    Returns:
        Transcription file in requested format
        
    Raises:
        404: Task or file not found
        400: Transcription not completed
    """
    if task_id not in tasks:
        raise HTTPException(404, "Task not found")
    
    if tasks[task_id].status != "completed":
        raise HTTPException(400, "Transcription not completed")
    
    if task_id not in task_files:
        raise HTTPException(404, "File not found")
    
    file_path = Path(task_files[task_id])
    
    if not file_path.exists():
        raise HTTPException(404, "Export file not found")
    
    from mimetypes import guess_type
    media_type, _ = guess_type(str(file_path))
    
    # Cleanup callback after download
    async def cleanup_files():
        """Clean up all files after successful download"""
        try:
            # Delete transcription file
            if file_path.exists():
                os.remove(file_path)
                logger.info(f"ğŸ§¹ Cleaned up transcription: {file_path.name}")
            
            # Delete uploaded file
            upload_dir = Path("./storage/uploads")
            for ext in ['.mp4', '.mp3', '.wav', '.m4a', '.avi', '.mov', '.flac', '.ogg']:
                upload_file = upload_dir / f"{task_id}{ext}"
                if upload_file.exists():
                    os.remove(upload_file)
                    logger.info(f"ğŸ§¹ Cleaned up upload: {upload_file.name}")
            
            # Delete processed audio
            processed_file = Path("./storage/processed") / f"{task_id}.wav"
            if processed_file.exists():
                os.remove(processed_file)
                logger.info(f"ğŸ§¹ Cleaned up processed audio: {processed_file.name}")
            
            # Remove from task storage
            tasks.pop(task_id, None)
            task_results.pop(task_id, None)
            task_files.pop(task_id, None)
            task_to_session.pop(task_id, None)
            
            logger.info(f"âœ¨ Complete cleanup for task {task_id[:8]}")
            
        except Exception as e:
            logger.warning(f"Cleanup error for {task_id}: {e}")
    
    return FileResponse(
        path=file_path,
        filename=file_path.name,
        media_type=media_type or "application/octet-stream",
        background=BackgroundTasks().add_task(cleanup_files)
    )